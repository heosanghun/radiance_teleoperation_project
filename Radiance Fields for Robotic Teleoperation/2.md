# Radiance Fields for Robotic Teleoperation 개발 계획서

## 프로젝트 개요
Neural Radiance Fields(NeRF)와 3D Gaussian Splatting(3DGS)을 활용한 고품질 실시간 로봇 원격 조작 시스템 개발

## Phase 1: 환경 설정 및 기초 구조 구축 (2-3주)

### 1.1 개발 환경 구축
**목표**: 필수 소프트웨어 스택 설치 및 검증
- [ ] **ROS 환경 설정**
  - ROS Noetic/Melodic 설치
  - Catkin workspace 생성
  - 필수 ROS 패키지 설치 (tf2, sensor_msgs, geometry_msgs)
  
- [ ] **NerfStudio 환경 구축**
  ```bash
  conda create --name nerfstudio -y python=3.8
  conda activate nerfstudio
  pip install --upgrade pip
  conda install pytorch==2.1.2 torchvision==0.16.2 pytorch-cuda=11.8 -c pytorch -c nvidia
  pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch
  ```

- [ ] **GPU 환경 검증**
  - CUDA 호환성 확인
  - tinyCudaNN 설치 및 테스트
  - 메모리 요구사항 확인 (최소 8GB VRAM)

### 1.2 기본 ROS 패키지 구조 생성
- [ ] **패키지 생성**
  ```bash
  catkin_create_pkg radiance_teleoperation std_msgs rospy roscpp sensor_msgs geometry_msgs tf2_ros
  ```
- [ ] **기본 디렉토리 구조**
  ```
  radiance_teleoperation/
  ├── src/
  ├── config/
  ├── launch/
  ├── scripts/
  └── msg/
  ```

### 1.3 하드웨어 연결 테스트
- [ ] **카메라 설정**
  - Intel RealSense 435i/L515 연결 테스트
  - 멀티 카메라 동기화 확인
  - 카메라 캘리브레이션 수행

## Phase 2: 데이터 수집 및 처리 파이프라인 구현 (3-4주)

### 2.1 멀티 카메라 데이터 수집 시스템
**목표**: 실시간 멀티 카메라 데이터 스트림 구현

- [ ] **Camera Node 개발**
  - ROS 노드로 카메라 데이터 스트리밍
  - 이미지, 깊이, 카메라 정보 토픽 발행
  - 동기화된 이미지 캡처 구현

- [ ] **Data Buffer 시스템**
  - 실시간 이미지 버퍼 관리
  - 메모리 효율적인 데이터 저장
  - 블러 감지 및 필터링

- [ ] **Configuration 시스템**
  ```yaml
  # config/camera_config.yaml
  cameras:
    - name: "front_camera"
      image_topic: "/camera/color/image_raw"
      depth_topic: "/camera/depth/image_rect_raw"
      info_topic: "/camera/color/camera_info"
      camera_frame: "camera_color_optical_frame"
      intrinsics:
        fx: 615.0
        fy: 615.0
        cx: 320.0
        cy: 240.0
  ```

### 2.2 데이터 전처리 파이프라인
- [ ] **Image Preprocessing**
  - 왜곡 보정
  - 색상 정규화
  - 해상도 조정

- [ ] **Transform 관리**
  - TF tree 구성
  - 카메라 포즈 추정
  - 좌표계 변환

## Phase 3: Radiance Field 통합 (4-5주)

### 3.1 NerfStudio-ROS 브릿지 구현
**목표**: NerfStudio와 ROS 간 실시간 데이터 교환

- [ ] **커스텀 Dataset 클래스**
  ```python
  class ROSDataset(InputDataset):
      def __init__(self, ros_buffer):
          self.ros_buffer = ros_buffer
          # ROS 데이터를 NerfStudio 형식으로 변환
  ```

- [ ] **실시간 DataLoader**
  - ROS 토픽에서 실시간 데이터 수신
  - NerfStudio 훈련 형식으로 변환
  - 배치 단위 데이터 준비

- [ ] **ActionServer 구현**
  - 렌더링 요청 처리
  - 클라이언트별 고유 ID 관리
  - 비동기 렌더링 지원

### 3.2 NeRF 및 3DGS 모델 구현
- [ ] **NeRF 모델 최적화**
  - 실시간 훈련을 위한 경량화
  - 점진적 해상도 훈련
  - 메모리 효율성 개선

- [ ] **3D Gaussian Splatting 구현**
  - 고속 렌더링 (151 FPS 목표)
  - 실시간 가우시안 최적화
  - 메모리 효율적인 스플랫 관리

### 3.3 온라인 학습 시스템
- [ ] **실시간 훈련 파이프라인**
  - 새로운 이미지 데이터 지속적 통합
  - 적응적 학습률 조정
  - 모델 수렴 모니터링

## Phase 4: 시각화 시스템 구현 (3-4주)

### 4.1 RViz 플러그인 개발
**목표**: 기존 ROS 시각화 시스템과 완전 통합

- [ ] **NerfViewController 플러그인**
  - RViz Views 패널 통합
  - 동적/연속 렌더 모드 지원
  - 점진적 해상도 렌더링 (10%→50%→100%)

- [ ] **깊이 기반 폐색 처리**
  - RViz 요소와의 깊이 비교
  - 동적 장면 구성
  - 바운딩 박스 기반 씬 크롭핑

### 4.2 VR 시스템 개발
**목표**: Meta Quest 3 기반 몰입형 인터페이스

- [ ] **Unity 프로젝트 설정**
  - Unity 2022.3.12f1 환경 구성
  - OpenXR 플러그인 설치
  - Meta Quest 3 타겟 설정

- [ ] **핵심 VR 컴포넌트 구현**
  - Hand tracking 시스템
  - Haptic feedback 구현
  - Palm menu 인터페이스

- [ ] **ROS-Unity 통신**
  - TCP Endpoint 연결
  - 실시간 데이터 스트리밍
  - 포즈 데이터 양방향 통신

## Phase 5: 로봇 플랫폼 통합 (2-3주)

### 5.1 다중 로봇 지원 시스템
- [ ] **로봇 모델 매니저**
  - Franka Panda (정적 암)
  - Anybotics Anymal (모바일 베이스)
  - DynaArm + Anymal (모바일 암)
  - 실시간 로봇 모델 전환

- [ ] **로봇별 최적화**
  - 각 로봇의 작업 공간 특성 고려
  - 센서 구성에 따른 파라미터 조정
  - 성능 벤치마크 및 최적화

### 5.2 제어 인터페이스 구현
- [ ] **Pose Publisher**
  - 목표 포즈 발행
  - 웨이포인트 시퀀스 관리
  - 충돌 회피 통합

## Phase 6: 성능 최적화 및 사용자 연구 (3-4주)

### 6.1 성능 최적화
**목표**: 논문 수준의 성능 달성

- [ ] **렌더링 속도 최적화**
  - 3DGS: 151 FPS 달성
  - NeRF: 실용적 속도로 개선
  - GPU 메모리 사용량 최소화

- [ ] **훈련 속도 최적화**
  - Iteration time: ~35ms 목표
  - Voxblox 품질 도달 시간: ~7초
  - 적응적 배치 크기 조정

### 6.2 품질 메트릭 구현
- [ ] **평가 시스템**
  - PSNR, SSIM, LPIPS 측정
  - 실시간 품질 모니터링
  - 자동 품질 보고서 생성

### 6.3 사용자 연구 설계
- [ ] **실험 프로토콜**
  - 20명 참가자 대상 연구
  - VR vs 2D 인터페이스 비교
  - 이동 작업 vs 조작 작업 평가

## Phase 7: 통합 테스트 및 배포 (2주)

### 7.1 시스템 통합 테스트
- [ ] **End-to-End 테스트**
  - 전체 파이프라인 검증
  - 다양한 환경에서 성능 테스트
  - 장시간 안정성 테스트

- [ ] **성능 벤치마크**
  - 논문 결과 재현 검증
  - 새로운 환경에서 성능 측정
  - 리소스 사용량 분석

### 7.2 문서화 및 배포
- [ ] **사용자 매뉴얼 작성**
  - 설치 가이드
  - 구성 가이드
  - 문제 해결 가이드

- [ ] **오픈소스 배포**
  - GitHub 저장소 정리
  - README.md 작성
  - 라이선스 설정
  - CI/CD 파이프라인 구성

## 개발 리소스 요구사항

### 하드웨어
- **개발 워크스테이션**: NVIDIA RTX 4080/4090 (12GB+ VRAM)
- **로봇 플랫폼**: Franka Panda 또는 Anymal (선택)
- **센서**: Intel RealSense 435i/L515
- **VR 헤드셋**: Meta Quest 3
- **네트워크**: 기가비트 이더넷

### 소프트웨어 스택
- **OS**: Ubuntu 18.04/20.04
- **ROS**: Noetic/Melodic
- **Python**: 3.8+
- **CUDA**: 11.8+
- **Unity**: 2022.3.12f1

## 위험 요소 및 완화 방안

### 기술적 위험
1. **GPU 메모리 부족**
   - 완화: 배치 크기 동적 조정, 모델 경량화
2. **실시간 성능 미달**
   - 완화: 점진적 최적화, 하드웨어 업그레이드
3. **ROS-Unity 통신 지연**
   - 완화: TCP 최적화, 로컬 네트워크 사용

### 일정 위험
1. **NerfStudio 통합 복잡성**
   - 완화: 단계별 구현, 전문가 컨설팅
2. **VR 개발 경험 부족**
   - 완화: Unity 교육, 기존 코드 활용

## 마일스톤 및 검증 기준

### 마일스톤 1 (4주): 기본 데이터 파이프라인
- 멀티 카메라 실시간 스트리밍
- ROS-NerfStudio 기본 통합

### 마일스톤 2 (8주): Radiance Field 렌더링
- NeRF/3DGS 실시간 훈련
- 기본 품질 메트릭 달성

### 마일스톤 3 (12주): VR 시각화
- Quest 3에서 동작하는 VR 인터페이스
- 핸드 트래킹 기반 상호작용

### 마일스톤 4 (16주): 시스템 완성
- 전체 시스템 통합
- 논문 수준 성능 달성
- 사용자 연구 완료

## 성공 기준
- **품질**: NeRF PSNR > 20, 3DGS PSNR > 25
- **속도**: 3DGS 렌더링 > 100 FPS
- **사용성**: VR 인터페이스 선호도 > 70%
- **안정성**: 1시간 연속 운영 가능

이 계획서는 논문의 연구 결과를 실용적인 시스템으로 구현하기 위한 단계별 로드맵을 제시합니다. 각 단계는 검증 가능한 결과물을 포함하며, 위험 요소를 사전에 식별하고 완화 방안을 제시했습니다.